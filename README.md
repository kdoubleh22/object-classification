# object-classification

## ğŸ“Œ ëª©ì°¨  
0. ìš”ì•½
1. ì‹¤í–‰ ë°©ë²•
2. API ì˜ˆì‹œ
3. ì£¼ìš” ì½”ë“œ ì„¤ëª… 
4. ë ˆí¼ëŸ°ìŠ¤

## 0. ìš”ì•½

![Image](https://github.com/user-attachments/assets/7da65b98-5b3d-41c7-aaa1-48f952808de4)

AIë¥¼ í™œìš©í•œ ì˜ì–´í•™ìŠµ ì•± í”„ë¡œì íŠ¸ ì¤‘ ì‚¬ë¬¼ ë¶„ë¥˜ API ë¶€ë¶„ì…ë‹ˆë‹¤. OpenAI CLIP ëª¨ë¸ í™œìš©í–ˆìŠµë‹ˆë‹¤.

## 1. ì‹¤í–‰ ë°©ë²•

```jsx
1. git clone.
git clone https://github.com/kdoubleh22/object-classification.git

2. í”„ë¡œì íŠ¸ í´ë”ë¡œ ì´ë™.
cd object-classification

3. docker ë¹Œë“œ.
docker build -t name .

4. docker run.
docker run -p 80:80 name
```

## 2. API ì˜ˆì‹œ

![Image](https://github.com/user-attachments/assets/e08c4d30-207d-4696-b2d5-a826e8b3a732)

## 3. ì£¼ìš” ì½”ë“œ ì„¤ëª…

### 3-1. [main.py](http://main.py) [1]

```python
model, preprocess = clip.load("ViT-B/32", device=device)
```

CLIPì˜ ì—¬ëŸ¬ ëª¨ë¸ ì¤‘ Vision Transformer êµ¬ì¡°, base í¬ê¸°, 32 íŒ¨ì¹˜ í¬ê¸°ì˜ ëª¨ë¸ì„ loadí•©ë‹ˆë‹¤. clip.available_models()ë¥¼ í†µí•´ ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px'] ìƒí™©ì— ë§ëŠ” ë‹¤ë¥¸ ëª¨ë¸ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
labels = ["airplane", "apple", "ball", "banana", "bicycle", "book", "broccoli", "burger", "bus",
            "cake", "candy", "cap", "cat", "chair", "chopsticks", "cookie", "crayon", "cup", "dinosaur",
            "dog", "duck", "eraser", "firetruck", "flower", "fork", "glasses", "grape", "icecream",
            "milk", "orange", "pencil", "penguin", "piano", "pizza", "policecar", "scissors",
            "socks", "spoon", "strawberry", "table", "tiger", "toothbrush", "tree", "television", "window"]
```

labelsì— ë¶„ë¥˜ë¥¼ ì›í•˜ëŠ” ë‹¨ì–´ë¥¼ ë„£ìŠµë‹ˆë‹¤. í”„ë¡œì íŠ¸ì˜ ë‹¨ì–´ì¥ì— ë“¤ì–´ê°ˆ ë‹¨ì–´ë¥¼ ì§€ì •í•´ì¤¬ìŠµë‹ˆë‹¤.

```python
text_inputs = torch.cat([clip.tokenize(f"a photo of a {c}") for c in labels]).to(device)
```

â€œa photo of aâ€ë¥¼ ë¶™ì´ëŠ” ì´ìœ ëŠ” ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.

â€œ

we found that using the prompt template â€œA photo of a {label}.â€ to be a good default that helps specify the text is about the content of the image. This often improves performance over the baseline of using only the label text. For instance, just using this prompt improves accuracy on ImageNet by 1.3%.

â€œ [2]

CLIPì´ ì‚¬ì „í•™ìŠµí•œ ë°ì´í„°ëŠ” (word, image)ê°€ ì•„ë‹Œ (text, image) ë°ì´í„° ìŒìœ¼ë¡œ í•™ìŠµí–ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

```python
with torch.no_grad():
    image_feature = model.encode_image(image)
    text_features = model.encode_text(text_inputs)
```

íŠ¹ì§• ë²¡í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

```python
image_feature /= image_feature.norm(dim=-1, keepdim=True)
text_features /= text_features.norm(dim=-1, keepdim=True)
similarity = (100.0 * image_feature @ text_features.T).softmax(dim=-1)
values, indices = similarity[0].topk(similarity.size(-1))
```

ì •ê·œí™”í•œ í›„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ êµ¬í•˜ê³  ë‚´ë¦¼ì°¨ìˆœí•©ë‹ˆë‹¤.

```python
for value, index in zip(values, indices):
print(f"{labels[index]:>15s} : {100 * value.item():.2f}%")
```

ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.

![Image](https://github.com/user-attachments/assets/b6a391af-536c-4bc1-952c-4d109448e87a)

## 4. ë ˆí¼ëŸ°ìŠ¤

[1] https://github.com/openai/CLIP

[2] https://arxiv.org/abs/2103.00020
